{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c70ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Final_Test.py\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F \n",
    "import os\n",
    "import shutil\n",
    "from timm import create_model\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score,accuracy_score,f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class STN(nn.Module):\n",
    "\n",
    "    def __init__(self, loc_model,device):\n",
    "        super(STN, self).__init__()\n",
    "\n",
    "        self.f_loc = loc_model\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x,s1=0.8,s2=0.6): \n",
    "\n",
    "        batch_images = x\n",
    "\n",
    "        theta = self.f_loc(x)\n",
    "\n",
    "        theta_x_y = theta.view(-1, 4)\n",
    "        affine_1 = torch.full([theta.size()[0],2,3],0.0)\n",
    "        #print(theta_x_y)\n",
    "        #print(affine_1.shape)\n",
    "        affine_1[:,0,0]=s1\n",
    "        affine_1[:,1,1]=s1\n",
    "        affine_1[:,0,2]=theta_x_y[:,0]\n",
    "        affine_1[:,1,2]=theta_x_y[:,1]\n",
    "        affine_1 = affine_1.to(self.device)#.cuda()\n",
    "        \n",
    "        affine_2 = torch.full([theta.size()[0],2,3],0.0)\n",
    "        affine_2[:,0,0]=s2\n",
    "        affine_2[:,1,1]=s2\n",
    "        affine_2[:,0,2]=theta_x_y[:,2]\n",
    "        affine_2[:,1,2]=theta_x_y[:,3]\n",
    "        affine_2 = affine_2.to(self.device)#.cuda()\n",
    "        \n",
    "        grid_1 = F.affine_grid(affine_1, batch_images.size())\n",
    "        rois_1 = F.grid_sample(batch_images, grid_1)\n",
    "        \n",
    "        grid_2 = F.affine_grid(affine_2, batch_images.size())\n",
    "        rois_2 = F.grid_sample(batch_images, grid_2)\n",
    "              \n",
    "        return batch_images,rois_1,rois_2,theta_x_y\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "def Get_STN(pkl_file,num_classes=219,device=\"cuda\"):\n",
    "    model_name = \"resnet50\"\n",
    "    STN_local = create_model(model_name, pretrained=False,num_classes=num_classes)\n",
    "    num_ftrs = STN_local.fc.in_features\n",
    "    STN_local.fc = torch.nn.Sequential(nn.Linear(num_ftrs, 256),\n",
    "                                       nn.Tanh(),\n",
    "                                        nn.Linear(256, 4), \n",
    "                                        nn.Tanh(),)\n",
    "    STN_net = STN(loc_model=STN_local,device = device)\n",
    "    STN_net.load_state_dict(torch.load(pkl_file,map_location=device))\n",
    "    return STN_net\n",
    "\n",
    "def Get_Model(model_name,pkl_path,num_classes=219):\n",
    "    model = create_model(model_name, pretrained=False,num_classes=num_classes, checkpoint_path=pkl_path)\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    name = \"new_Backbone_SwinV2_two_one_swin_L_outpust\"\n",
    "    \n",
    "    template = pd.read_csv('./AI_CUP_2022_test/submission_template.csv')\n",
    "    template.loc[:,'category']=-1\n",
    "    \n",
    "    #identified_template = pd.read_csv('./submission_template.csv')\n",
    "    #identified = identified_template.loc[template['category']!=-1].loc[:,'filename'].tolist()\n",
    "    \n",
    "    #val_path = r\"C:\\Users\\chen_hung\\Desktop\\AI_CUP_2022\\Datasets\\training\\val\"\n",
    "    \n",
    "    orchid_test_set = \"./AI_CUP_2022_test/orchid_set\"\n",
    "    \n",
    "    #******************************************************************************************************************************\n",
    "    BATCH_SIZE = 1\n",
    "    Num_workers = 2\n",
    "    \n",
    "    resize = 384\n",
    "    mean_384_one_fold = [0.5075337404927281 ,0.45864544276917535 ,0.4169235386212412] \n",
    "    std_384_one_fold = [0.2125643051799512 ,0.2385082849964861 ,0.22386483801695406]\n",
    "    mean_384_two_fold = [0.45625534556274666, 0.4220624936173144, 0.3649616601198825]\n",
    "    std_384_two_fold = [0.2143212828816861, 0.2210437745632625, 0.2062174242104951]\n",
    "    \n",
    "    #mean = mean_384_one_fold\n",
    "    #std = std_384_one_fold\n",
    "    \n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(resize, interpolation=Image.BICUBIC),\n",
    "        transforms.CenterCrop(resize),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    tfm_one = transforms.Normalize(mean_384_one_fold, std_384_one_fold)\n",
    "    tfm_two = transforms.Normalize(mean_384_two_fold, std_384_two_fold)\n",
    "    \n",
    "    orchid_test_path = ImageFolderWithPaths(orchid_test_set,transform=tfm)\n",
    "    orchid_test_Loader_path = torch.utils.data.DataLoader(dataset=orchid_test_path,batch_size=BATCH_SIZE,shuffle=False,num_workers=Num_workers)\n",
    "    \n",
    "    #val_data = datasets.ImageFolder(val_path)\n",
    "    #print(val_data.classes)\n",
    "    #print(orchid_test_path.calsses)\n",
    "    #*************************************************************************************************************************************************\n",
    "    one_fold_STN_pkl = \"./pytorch-image-models/save_STN/one_fold_save_STN_net_best_37.3.454599573160522e-05.pkl\"\n",
    "    two_fold_STN_pkl = \"./pytorch-image-models/save_STN/two_fold_save_STN_net_best_18.2.972046786453575e-05.pkl\"\n",
    "    \n",
    "    model_name = \"swinv2_base_window12to24_192to384_22kft1k\"\n",
    "    one_fold_Network_pkl = \"./pytorch-image-models/output/0531_0310_swinv2_base_window12to24_192to384_22kft1k_official_aug_v0/model_best.pth.tar\"\n",
    "    two_fold_Network_pkl = \"./pytorch-image-models/output/0603_0131_swinv2_base_window12to24_192to384_22kft1k_complement_final_bacbbone_95.5927_219epoch/model_best.pth.tar\"\n",
    "    \n",
    "    #one_fold_STN_Network_pkl = \"./pytorch-image-models/output/STN_backbone_official_86_new_V2/checkpoint-37.pth.tar\"\n",
    "    two_fold_STN_Network_pkl = \"\"\n",
    "    \n",
    "    \n",
    "    Swin_L_pkl = \"./pytorch-image-models/output/0__swin_large__official_95.2887/model_best.pth.tar\"\n",
    "    Swin_L_model = Get_Model('swin_large_patch4_window12_384_in22k',pkl_path=Swin_L_pkl)\n",
    "    Swin_L_model = nn.DataParallel(Swin_L_model)\n",
    "    Swin_L_model = Swin_L_model.cuda()\n",
    "    \n",
    "    STN_one_model = Get_STN(pkl_file=one_fold_STN_pkl)\n",
    "    STN_two_model = Get_STN(pkl_file=two_fold_STN_pkl)\n",
    "    \n",
    "    Network_one_model = Get_Model(model_name,pkl_path=one_fold_Network_pkl)\n",
    "    Network_two_model = Get_Model(model_name,pkl_path=two_fold_Network_pkl)\n",
    "    \n",
    "    #STN_Network_one_model = Get_Model(model_name,pkl_path=one_fold_STN_Network_pkl)\n",
    "    \n",
    "    STN_one_model = nn.DataParallel(STN_one_model)\n",
    "    STN_one_model = STN_one_model.cuda()\n",
    "    STN_two_model = nn.DataParallel(STN_two_model)\n",
    "    STN_two_model = STN_two_model.cuda()\n",
    "    \n",
    "    Network_one_model = nn.DataParallel(Network_one_model)\n",
    "    Network_one_model = Network_one_model.cuda()\n",
    "    Network_two_model = nn.DataParallel(Network_two_model)\n",
    "    Network_two_model = Network_two_model.cuda()\n",
    "    \n",
    "    #STN_Network_one_model = nn.DataParallel(STN_Network_one_model)\n",
    "    #STN_Network_one_model = STN_Network_one_model.cuda()\n",
    "    \n",
    "    #*************************************************************************************************************************************************\n",
    "    not_ok = []\n",
    "    ok_count = 0\n",
    "    with torch.no_grad():\n",
    "            STN_one_model.eval()\n",
    "            STN_two_model.eval()\n",
    "            Network_one_model.eval()\n",
    "            Network_two_model.eval()\n",
    "            #STN_Network_one_model.eval()\n",
    "            Swin_L_model.eval()\n",
    "            for step, (batch_x,label_y,path) in enumerate(orchid_test_Loader_path):\n",
    "                img_file = os.path.split(path[0])[1]\n",
    "                \n",
    "                #if not(img_file in identified):\n",
    "                test_one = Variable(tfm_one(batch_x)).cuda()\n",
    "                test_two = Variable(tfm_two(batch_x)).cuda()\n",
    "                \n",
    "                #roi_0_one,roi_1_one,roi_2_one,_ = STN_one_model(test_one)\n",
    "                #roi_0_two,roi_1_two,roi_2_two,_ = STN_two_model(test_two)\n",
    "                \n",
    "                outpust_0_one = Network_one_model(test_one)\n",
    "                #outpust_1_one = STN_Network_one_model(roi_1_one)\n",
    "                #outpust_2_one = STN_Network_one_model(roi_2_one)\n",
    "                \n",
    "                outpust_0_two = Network_two_model(test_two)\n",
    "                #outpust_1_two = Network_two_model(roi_1_two)\n",
    "                #outpust_2_two = Network_two_model(roi_2_two)\n",
    "                \n",
    "                output_L = Swin_L_model(test_one)\n",
    "                \n",
    "                final_outputs = F.softmax(outpust_0_one, dim=1)+F.softmax(outpust_0_two, dim=1)+F.softmax(output_L, dim=1)\n",
    "                \n",
    "                #+F.softmax(outpust_1_one, dim=1)+F.softmax(outpust_2_one, dim=1)\n",
    "                \n",
    "                #final_outputs = F.softmax(outpust_0_one, dim=1)+F.softmax(outpust_1_one, dim=1)+F.softmax(outpust_2_one, dim=1)+\n",
    "                #            F.softmax(outpust_0_two, dim=1)+F.softmax(outpust_1_two, dim=1)+F.softmax(outpust_2_two, dim=1)\n",
    "                            \n",
    "                ans = torch.max(final_outputs,1)[1].squeeze()\n",
    "                final_ans = int(ans)\n",
    "                #print(\"{}/{},{}\".format(step,len(orchid_test_Loader_path),img_file))\n",
    "                \n",
    "                try:\n",
    "                    template.loc[template['filename']==img_file,'category']=final_ans\n",
    "                    ok_count+=1\n",
    "                except:\n",
    "                    print(\"{}/{} is not ok\".format(label_y,img_file) )\n",
    "                    not_ok.append([label_y,img_file])\n",
    "                if(step%10000==0):\n",
    "                    template.to_csv(\"./Ans/{}_outpust_interrupt_{}.csv\".format(name,step), index=False)\n",
    "                    print(\"{}/{},{}\".format(step,len(orchid_test_Loader_path),img_file))\n",
    "            \n",
    "            print(\"{}/{}\".format(ok_count,step))\n",
    "\n",
    "    \n",
    "    template.to_csv(\"./Ans/{}_outpust.csv\".format(name), index=False)\n",
    "    \n",
    "    for i in not_ok:\n",
    "        t_fi= open('/Ans/Not_ok.txt'.format(epoch+1),'w')\n",
    "        t_fi.writelines(\"{}/{} \\n\".format(i[0],i[1]))\n",
    "        t_fi.close()\n",
    "    #img_file = \"dmlg79vsu0.jpg\"\n",
    "    #model_ans = 1\n",
    "    #template.loc[template['filename']==img_file,'category']=model_ans\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118a51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
